model_name_or_path: "Vikhrmodels/Phikhr-14B-Instruct-R-19-12-24-SFT"
dataset: "hivaze/phi4-sft-po-dataset"
per_device_train_batch_size: 1
per_device_eval_batch_size: 2
num_train_epochs: 2
log_level: "info"
attn_implementation: "sdpa"
save_strategy: "steps"
save_steps: 50
save_total_limit: 12
learning_rate: 0.00002
lr_scheduler_type: cosine
gradient_accumulation_steps: 16
gradient_checkpointing: True
logging_steps: 1
remove_unused_columns: True
dataloader_num_workers: 2
dataset_num_proc: 20
max_length: 10240
max_prompt_length: 5120
save_only_model: True
generate_eval_examples: True
generate_during_eval: False
test_size: 0.05
evaluation_strategy: "steps"
eval_steps: 50
run_name: "smpo-smooth_lower_bound-phikr-14b-19-12-24-lora-196-b1.1-mm0.3-sft1.0-ltp0.02-mlp2.3-2"
output_dir: "checkpoints/smpo-smooth_lower_bound-phikr-14b-19-12-24-lora-196-b1.1-mm0.3-sft1.0-ltp0.02-mlp2.3-2"
warmup_steps: 10
report_to: "wandb"
beta: 1.1
margin_min: 0.3
margin_delta: 0.2
chosen_sft_ratio: 1.0
loss_type: "smooth_lower_bound"
lower_clip_percentile: 0.02
min_log_prob: -2.3
bf16: True
seed: 42
logging_first_step: True
use_peft: True
lora_task_type: CAUSAL_LM
lora_target_modules:
  - "qkv_proj"
  - "o_proj"
  - "gate_up_proj"
  - "down_proj"
  - "lm_head"
lora_r: 256
lora_alpha: 256
pad_token: "<|dummy_0|>"
eos_token: "<|im_end|>"
chat_template: "{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|im_start|>' + message['role'] + '<|im_sep|>'+ message['content'] | trim + '<|im_end|>' %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant<|im_sep|>' }}{% endif %}"
force_chat_template: False