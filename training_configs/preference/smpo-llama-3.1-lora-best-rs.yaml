model_name_or_path: "Vikhrmodels/Vikhr-Llama3.1-8B-Instruct-R-01-09-24"
dataset: "data/vikhr-llama-3.1-rm4-scored-answers-rs7-nw2-t0.8-pref-fixed.jsonl"
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
num_train_epochs: 2
log_level: "info"
attn_implementation: "sdpa"
save_strategy: "steps"
save_steps: 50
save_total_limit: 20
learning_rate: 0.00002
lr_scheduler_type: cosine
gradient_accumulation_steps: 8
gradient_checkpointing: True
logging_steps: 1
remove_unused_columns: True
dataloader_num_workers: 2
dataset_num_proc: 10
max_length: 8192
max_prompt_length: 4096
save_only_model: True
generate_eval_examples: True
test_size: 0.05
evaluation_strategy: "steps"
eval_steps: 50
run_name: "smpo-smooth_lower_bound-top-rs-llama-3.1-01-09-24-lora-96-qkvogud-b1.1-mm0.4-sft0.75-fixed"
output_dir: "models/smpo-smooth_lower_bound-top-rs-llama-3.1-01-09-24-lora-96-qkvogud-b1.1-mm0.4-sft0.75-fixed"
warmup_steps: 10
report_to: "wandb"
beta: 1.1
margin_min: 0.4
margin_delta: 0.2
chosen_sft_ratio: 0.75
loss_type: "smooth_lower_bound"
bf16: False
fp16: True
seed: 42
logging_first_step: True
use_peft: True
lora_task_type: CAUSAL_LM
lora_target_modules:
  - "k_proj"
  - "v_proj"
  - "q_proj"
  - "o_proj"
  - "gate_proj"
  - "up_proj"
  - "down_proj"
lora_r: 96
lora_alpha: 96
pad_token: "<|reserved_special_token_0|>"
eos_token: "<|eot_id|>"
chat_template: "{{ bos_token }}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n\n' }}{% endif %}"
force_chat_template: False